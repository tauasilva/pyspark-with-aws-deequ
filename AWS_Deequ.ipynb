{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepara ambiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para este teste estou usando a versão spark-3.1.2-bin-hadoop3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME'] = '/c/spark-3.1.2-bin-hadoop3.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instala o pydeequ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/awslabs/python-deequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydeequ in /home/taua/anaconda3/envs/aws_emr_deploy379/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.14.1 in /home/taua/anaconda3/envs/aws_emr_deploy379/lib/python3.7/site-packages (from pydeequ) (1.21.4)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /home/taua/anaconda3/envs/aws_emr_deploy379/lib/python3.7/site-packages (from pydeequ) (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/taua/anaconda3/envs/aws_emr_deploy379/lib/python3.7/site-packages (from pandas>=0.23.0->pydeequ) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/taua/anaconda3/envs/aws_emr_deploy379/lib/python3.7/site-packages (from pandas>=0.23.0->pydeequ) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/taua/anaconda3/envs/aws_emr_deploy379/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=0.23.0->pydeequ) (1.16.0)\n",
      "Requirement already satisfied: pyspark in /home/taua/anaconda3/envs/aws_emr_deploy379/lib/python3.7/site-packages (3.1.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in /home/taua/anaconda3/envs/aws_emr_deploy379/lib/python3.7/site-packages (from pyspark) (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install pydeequ\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please set env variable SPARK_VERSION\n",
      "21/12/07 09:23:11 WARN Utils: Your hostname, DBC-0001023 resolves to a loopback address: 127.0.1.1; using 172.27.112.1 instead (on interface eth3)\n",
      "21/12/07 09:23:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/c/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/c/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/taua/.ivy2/cache\n",
      "The jars for the packages stored in: /home/taua/.ivy2/jars\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a9cceebf-c6a1-4605-a612-c8de5fe956c5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;1.2.2-spark-3.0 in central\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.1 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      ":: resolution report :: resolve 1248ms :: artifacts dl 226ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;1.2.2-spark-3.0 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.1 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   1   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a9cceebf-c6a1-4605-a612-c8de5fe956c5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/42ms)\n",
      "21/12/07 09:23:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "\n",
    "import pydeequ\n",
    "\n",
    "spark = (SparkSession\n",
    "            .builder\n",
    "            .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "            .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "            .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executando um teste para verificar se a sessão spark esta correta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coluna1</th>\n",
       "      <th>valor</th>\n",
       "      <th>quantidade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Banana</td>\n",
       "      <td>1.50</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Maça</td>\n",
       "      <td>1.85</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Laranja</td>\n",
       "      <td>3.00</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   coluna1  valor  quantidade\n",
       "0   Banana   1.50         5.0\n",
       "1     Maça   1.85         6.0\n",
       "2  Laranja   3.00         NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = spark.sparkContext.parallelize([\n",
    "    Row(coluna1=\"Banana\",  valor=1.50, quantidade=5),\n",
    "    Row(coluna1=\"Maça\",    valor=1.85, quantidade=6),\n",
    "    Row(coluna1=\"Laranja\", valor=3.00, quantidade=None)]).toDF()\n",
    "\n",
    "\n",
    "df_test.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora que sabemos que nossa sessão spark esta correta, vamos importar um fonte de dados e criar nosso dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando dados "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este exemplo vou utilizar um parte dos dados de empresas brasileiras.\n",
    "Estes dados são disponibilizados em .zip pelo governo federal\n",
    "\n",
    "https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/cadastros/consultas/dados-publicos-cnpj\n",
    "\n",
    "São vários zips, para facilitar salver um dos arquivos sem ./fonte_de_dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"cnpj_basico\",StringType(),True), \\\n",
    "    StructField(\"razao_social\",StringType(),True), \\\n",
    "    StructField(\"natureza_juridica\",StringType(),True), \\\n",
    "    StructField(\"qualificacao_do_responsavel\", StringType(), True), \\\n",
    "    StructField(\"capital_social\", StringType(), True), \\\n",
    "    StructField(\"porte_da_empresa\", StringType(), True),\n",
    "    StructField(\"ente_federativo_responsavel\", StringType(), True),\n",
    "  ])\n",
    "\n",
    "\n",
    "#detalhes sobre o layout em https://www.gov.br/receitafederal/pt-br/assuntos/orientacao-tributaria/cadastros/consultas/arquivos/novolayoutdosdadosabertosdocnpj-dez2021.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------+--------------------+---------------------------+--------------+----------------+---------------------------+\n",
      "|          cnpj_basico|        razao_social|   natureza_juridica|qualificacao_do_responsavel|capital_social|porte_da_empresa|ente_federativo_responsavel|\n",
      "+---------------------+--------------------+--------------------+---------------------------+--------------+----------------+---------------------------+\n",
      "| PK\u0003\u0004\u0014\u0000\u0000\u0000�z\u0010SmU�...|                null|                null|                       null|          null|            null|                       null|\n",
      "| \u0017����Mr�|�\u0000\u0017H�\u0005x�...|                null|                null|                       null|          null|            null|                       null|\n",
      "| ,y+3��:\u001f:�\u0014��4��\u0006...|                null|                null|                       null|          null|            null|                       null|\n",
      "| ?��\u0012�U�f�m�`Ɏ�tX�...|                null|                null|                       null|          null|            null|                       null|\n",
      "|    \u0002�A�c��\u001b�r\u0016|�6\t\u001ex|                null|                null|                       null|          null|            null|                       null|\n",
      "| �����\u0017,J��op<hx�>...|                null|                null|                       null|          null|            null|                       null|\n",
      "| hۚ�8\u000b\u001a\\���\u0014�e1\tzy...|                null|                null|                       null|          null|            null|                       null|\n",
      "|                 g\u0005�D|[�bU[�\u001ei�\u000e�y\u001e$�O\u0014...|                null|                       null|          null|            null|                       null|\n",
      "| k�y�@.O2Ë\f��&���>...|                null|                null|                       null|          null|            null|                       null|\n",
      "| �I\"����\f�I\u001e�'��\u0016&...|��<��Fg�mQ��f���g...|                   k|                       null|          null|            null|                       null|\n",
      "|                  ���|                null|                null|                       null|          null|            null|                       null|\n",
      "|�j���鮒�c|�����\u0007�2...|                null|                null|                       null|          null|            null|                       null|\n",
      "| �!�:YO�N\u0010��.\u0011%\u0019=ϸ...|�(���\u0016�1>��N���I...||z�rA)�bﶤ�yE�\u0005��I...|                       null|          null|            null|                       null|\n",
      "| q�k���x�v���-��0\u0018...|                null|                null|                       null|          null|            null|                       null|\n",
      "| ���%�>���r�F&��E�...|                null|                null|                       null|          null|            null|                       null|\n",
      "|   �����~��>\u0010wwt�x\u0010F�|                null|                null|                       null|          null|            null|                       null|\n",
      "| ��:����S��q��\u000e�}P...|\u0000�tv���ͻ�-��g�©�d...|l�!��z\u0000�UK�W�v\u0004�...|                       null|          null|            null|                       null|\n",
      "| \u0018��A�M.IӮ/��_\u001d\u001d��...|                null|                null|                       null|          null|            null|                       null|\n",
      "| 8�5\u001b0��1>�Ǐ��+�ޓ�...|                null|                null|                       null|          null|            null|                       null|\n",
      "| o�4h`�\\�\u0002\u000e߷t��@�...|U �Z��F�\u000eA6X�N`��...|                null|                       null|          null|            null|                       null|\n",
      "+---------------------+--------------------+--------------------+---------------------------+--------------+----------------+---------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "df_dados_de_empresa = spark.read.csv('./fonte_de_dados/',sep=';',schema=schema)\n",
    "df_dados_de_empresa.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando o dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+---------------------------+--------------------+--------------------+---------------------------+\n",
      "|summary|         cnpj_basico|        razao_social|   natureza_juridica|qualificacao_do_responsavel|      capital_social|    porte_da_empresa|ente_federativo_responsavel|\n",
      "+-------+--------------------+--------------------+--------------------+---------------------------+--------------------+--------------------+---------------------------+\n",
      "|  count|              530545|              185662|               65729|                      23732|                8604|                3164|                       1164|\n",
      "|   mean|           21881.425|   5.261654135338346|   6.571428571428571|          7.769230769230769|   5.888888888888889|                 8.0|                        9.0|\n",
      "| stddev|   391311.5359487437|   8.003453059740613|    9.33143237944212|         10.017932638971695|   2.666666666666667|  1.4142135623730951|                       null|\n",
      "|    min|                   \u0000|                   \u0000|                   \u0000|                          \u0000|                   \u0000|\u0000(dFY\u0005R!f\u0003��g�l(]...|       \u0000B�\u0018}��*�A7k��xQ|...|\n",
      "|    max|􏄢\u001eS�R¦\u0010\f�H�\f\u0017�\u001e:...|􌵡��I�\u0016U\u000b+��ùr|6W\u0000�|󺈞�����V6��Y9�\u001cD2...|       􍨽\u0015`���D��d�X7A-\u0013...|􏾙�+�\u0007�1\u0006�\".?�/q\u0013...|񓢛�I���~ �\u0012t�6m��...|       �������\u001a��\u001b��in�,...|\n",
      "+-------+--------------------+--------------------+--------------------+---------------------------+--------------------+--------------------+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dados_de_empresa.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos criar uma função para validar o CNPJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "LENGTH_CNPJ = 14\n",
    "\n",
    "def is_cnpj_valido(cnpj: str) -> bool:\n",
    "    if len(cnpj) != LENGTH_CNPJ:\n",
    "        return False\n",
    "\n",
    "    if cnpj in (c * LENGTH_CNPJ for c in \"1234567890\"):\n",
    "        return False\n",
    "\n",
    "    cnpj_r = cnpj[::-1]\n",
    "    for i in range(2, 0, -1):\n",
    "        cnpj_enum = zip(cycle(range(2, 10)), cnpj_r[i:])\n",
    "        dv = sum(map(lambda x: int(x[1]) * x[0], cnpj_enum)) * 10 % 11\n",
    "        if cnpj_r[i - 1:i] != str(dv % 10):\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agora vamos criar uma nova coluna que vai receber o resultado da validação do CNPJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "is_cnpj_valido_lambda = udf(lambda x: is_cnpj_valido(x), StringType())\n",
    "df_dados_de_empresa = df_dados_de_empresa.withColumn(\"cnpj_valido\",is_cnpj_valido_lambda(col('cnpj_basico')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we define checks on the data, we want to calculate some statistics on the dataset; we call them metrics. As with Deequ, PyDeequ supports a rich set of metrics. For more information, see Test data quality at scale with Deequ or the GitHub repo. In the following example, we use the AnalysisRunner to capture the metrics you’re interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------+--------------------+\n",
      "| entity|            instance|        name|               value|\n",
      "+-------+--------------------+------------+--------------------+\n",
      "| Column|qualificacao_do_r...|Completeness|0.044543964923410735|\n",
      "| Column|    porte_da_empresa|Completeness|0.005938694801014308|\n",
      "| Column|        razao_social|Completeness|   0.348479757947509|\n",
      "| Column|   natureza_juridica|Completeness| 0.12337056592157694|\n",
      "|Dataset|                   *|        Size|            532777.0|\n",
      "| Column|         cnpj_basico|Completeness|  0.9958106299633805|\n",
      "| Column|      capital_social|Completeness| 0.01614934578632336|\n",
      "| Column|ente_federativo_r...|Completeness|0.002184778997591863|\n",
      "+-------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.analyzers import *\n",
    "analysisResult = AnalysisRunner(spark) \\\n",
    "    .onData(df_dados_de_empresa) \\\n",
    "    .addAnalyzer(Size()) \\\n",
    "    .addAnalyzer(Completeness(\"cnpj_basico\")) \\\n",
    "    .addAnalyzer(Completeness(\"razao_social\")) \\\n",
    "    .addAnalyzer(Completeness(\"natureza_juridica\")) \\\n",
    "    .addAnalyzer(Completeness(\"qualificacao_do_responsavel\")) \\\n",
    "    .addAnalyzer(Completeness(\"capital_social\")) \\\n",
    "    .addAnalyzer(Completeness(\"porte_da_empresa\")) \\\n",
    "    .addAnalyzer(Completeness(\"ente_federativo_responsavel\")) \\\n",
    "    .run()\n",
    "analysisResult_df = AnalyzerContext.successMetricsAsDataFrame(spark, analysisResult)\n",
    "analysisResult_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com base em uma analise inicial do dataframe, já podemos observar que existem problemas nos campos de porte_da_empresa e ente_federativo_responsavel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros com ente_federativo_responsavel igual a null: 531613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros com porte_da_empresa igual a null: 529613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "print(f'Registros com ente_federativo_responsavel igual a null: {df_dados_de_empresa.filter(df_dados_de_empresa.ente_federativo_responsavel.isNull()).count()}')\n",
    "print(f'Registros com porte_da_empresa igual a null: {df_dados_de_empresa.filter(df_dados_de_empresa.porte_da_empresa.isNull()).count()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and running tests for data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "After analyzing and understanding the data, we want to verify that the properties we have derived also hold for new versions of the dataset. By defining assertions on the data distribution as part of a data pipeline, we can ensure that every processed dataset is of high quality, and that any application consuming the data can rely on it.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algumas validações que vamos adicionar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "CNPJ preenchido e unico\n",
    "\n",
    "Razão Social preenchido\n",
    "\n",
    "Código do porte da empresa precisa ser algum código válido:\n",
    "    00  – NÃO INFORMADO \n",
    "    01  -  MICRO EMPRESA \n",
    "    03 - EMPRESA DE PEQUENO PORTE \n",
    "    05 - DEMAIS\n",
    "    \n",
    "Se o CNPJ é válido de acordo com a função que criamos    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Callback server started!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/07 09:24:22 ERROR Executor: Exception in task 4.0 in stage 12.0 (TID 54)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3281/1610615853.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in is_cnpj_valido\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: '�'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/07 09:24:22 ERROR Executor: Exception in task 0.0 in stage 12.0 (TID 50)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3281/1610615853.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in is_cnpj_valido\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: ' '\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/07 09:24:22 ERROR Executor: Exception in task 5.0 in stage 12.0 (TID 55)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3281/1610615853.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 6, in is_cnpj_valido\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/07 09:24:22 ERROR Executor: Exception in task 1.0 in stage 12.0 (TID 51)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3281/1610615853.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 6, in is_cnpj_valido\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/07 09:24:22 ERROR Executor: Exception in task 2.0 in stage 12.0 (TID 52)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3281/1610615853.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in is_cnpj_valido\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: 'Z'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/07 09:24:22 ERROR Executor: Exception in task 3.0 in stage 12.0 (TID 53)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3281/1610615853.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in is_cnpj_valido\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: '�'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/07 09:24:22 ERROR Executor: Exception in task 7.0 in stage 12.0 (TID 57)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3281/1610615853.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in is_cnpj_valido\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: '�'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/07 09:24:22 ERROR Executor: Exception in task 6.0 in stage 12.0 (TID 56)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3281/1610615853.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in is_cnpj_valido\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: '\\x02'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "21/12/07 09:24:22 WARN TaskSetManager: Lost task 5.0 in stage 12.0 (TID 55) (172.27.112.1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3281/1610615853.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 6, in is_cnpj_valido\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "21/12/07 09:24:22 ERROR TaskSetManager: Task 5 in stage 12.0 failed 1 times; aborting job\n",
      "21/12/07 09:24:22 WARN TaskSetManager: Lost task 3.0 in stage 12.0 (TID 53) (172.27.112.1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3281/1610615853.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in is_cnpj_valido\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: '�'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "21/12/07 09:24:22 WARN TaskSetManager: Lost task 0.0 in stage 12.0 (TID 50) (172.27.112.1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3281/1610615853.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in is_cnpj_valido\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: ' '\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "21/12/07 09:24:22 WARN TaskSetManager: Lost task 6.0 in stage 12.0 (TID 56) (172.27.112.1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3281/1610615853.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in is_cnpj_valido\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: '\\x02'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "21/12/07 09:24:22 WARN TaskSetManager: Lost task 2.0 in stage 12.0 (TID 52) (172.27.112.1 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n",
      "    process()\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/c/spark-3.1.2-bin-hadoop3.2/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3281/1610615853.py\", line 4, in <lambda>\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in is_cnpj_valido\n",
      "  File \"/tmp/ipykernel_3281/731073180.py\", line 15, in <lambda>\n",
      "ValueError: invalid literal for int() with base 10: 'Z'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:84)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:67)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>check_level</th>\n",
       "      <th>check_status</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Warning</td>\n",
       "      <td>SizeConstraint(Size(None))</td>\n",
       "      <td>Failure</td>\n",
       "      <td>org.apache.spark.SparkException: Job aborted d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Warning</td>\n",
       "      <td>CompletenessConstraint(Completeness(cnpj_basic...</td>\n",
       "      <td>Failure</td>\n",
       "      <td>org.apache.spark.SparkException: Job aborted d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Warning</td>\n",
       "      <td>CompletenessConstraint(Completeness(razao_soci...</td>\n",
       "      <td>Failure</td>\n",
       "      <td>org.apache.spark.SparkException: Job aborted d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Warning</td>\n",
       "      <td>UniquenessConstraint(Uniqueness(List(cnpj_basi...</td>\n",
       "      <td>Failure</td>\n",
       "      <td>Value: 0.9712107361298288 does not meet the co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Warning</td>\n",
       "      <td>ComplianceConstraint(Compliance(porte_da_empre...</td>\n",
       "      <td>Failure</td>\n",
       "      <td>org.apache.spark.SparkException: Job aborted d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>Warning</td>\n",
       "      <td>Warning</td>\n",
       "      <td>ComplianceConstraint(Compliance(cnpj_valido co...</td>\n",
       "      <td>Failure</td>\n",
       "      <td>org.apache.spark.SparkException: Job aborted d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          check check_level check_status  \\\n",
       "0  Review Check     Warning      Warning   \n",
       "1  Review Check     Warning      Warning   \n",
       "2  Review Check     Warning      Warning   \n",
       "3  Review Check     Warning      Warning   \n",
       "4  Review Check     Warning      Warning   \n",
       "5  Review Check     Warning      Warning   \n",
       "\n",
       "                                          constraint constraint_status  \\\n",
       "0                         SizeConstraint(Size(None))           Failure   \n",
       "1  CompletenessConstraint(Completeness(cnpj_basic...           Failure   \n",
       "2  CompletenessConstraint(Completeness(razao_soci...           Failure   \n",
       "3  UniquenessConstraint(Uniqueness(List(cnpj_basi...           Failure   \n",
       "4  ComplianceConstraint(Compliance(porte_da_empre...           Failure   \n",
       "5  ComplianceConstraint(Compliance(cnpj_valido co...           Failure   \n",
       "\n",
       "                                  constraint_message  \n",
       "0  org.apache.spark.SparkException: Job aborted d...  \n",
       "1  org.apache.spark.SparkException: Job aborted d...  \n",
       "2  org.apache.spark.SparkException: Job aborted d...  \n",
       "3  Value: 0.9712107361298288 does not meet the co...  \n",
       "4  org.apache.spark.SparkException: Job aborted d...  \n",
       "5  org.apache.spark.SparkException: Job aborted d...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "check = Check(spark, CheckLevel.Warning, \"Review Check\")\n",
    "\n",
    "checkResult = VerificationSuite(spark)\\\n",
    "    .onData(df_dados_de_empresa)\\\n",
    "    .addCheck(check.hasSize(lambda x: x >= 3)\\\n",
    "        .isComplete(\"cnpj_basico\")\\\n",
    "        .isComplete(\"razao_social\")\\\n",
    "        .isUnique(\"cnpj_basico\")\\\n",
    "        .isContainedIn(\"porte_da_empresa\", [\"00\", \"01\", \"03\", \"05\"]) \\\n",
    "        .isContainedIn(\"cnpj_valido\", ['true']))\\\n",
    "    .run()\n",
    "\n",
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Conseguimos importar uma fonte de dados e fazer uma analise e validação inicial. Isso pode contribuir para a qualidade dos dados que estamos trabalhando.\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
